# Confounding in Opioid Safety Studies

## What we are studying
In the field of opioids research we often want to be able to draw firm conclusions about what causes overdoses. We use data sources from electronic health records, insurance claims, vital statistics, and elsewhere to answer fundamental questions about the safety of medications. But we know that these data sources are imperfect, and that not everything is documented in them with certainty.
So how do we know whether what we are seeing is real? There is a widely accepted approach in epidemiology to test how robust our findings are by creating scenarios, some normal some extreme, that mimic what results would look like if a certain portion of the data was incorrectly specified. These are called sensitivity analysis. They allow us to identify where the bias might be in our findings. In order to do the simulations we need some rules that can be consistently applied to different datasets and research questions. In this project we are creating a set of open source and freely available tools that would allow any researcher to stress test their findings.

## Why it matters
We are evaluating and comparing methods for confounding by “indication”: the expected likelihood that patients receiving ADF opioids have different risk profiles for experiencing overdose than patients prescribed traditional opioids. ADF Category 1, 2, or 3 labeling is expected to change clinical practice, influencing interpretation of changes in rates from other observational data. For instance, under the counterfactual framework, the choice of comparators could be reconceptualized as a decision to best balance the underlying patient populations’ baseline risk of abuse between ADF and comparator, instead of solely relying on a comparison by active ingredient. 

In addition, since an individual’s risk for misuse of opioids likely changes over time, time-varying models will be deployed to quantify the importance of time-dependent adjustment. Advancing methodologic rigor of methods for controlling for confounding by indication is critical to achieving valid results from large administrative databases. 

## How we are studying it
We will quantify the extent to which confounding by “indication” is observable in claims and EHR data. To do so, we developing a directed acyclic graph (DAG) that will guide our choice of metrics to identify confounding by “indication.” We will make these public, using Daggity.

Longitudinal data necessitate accounting for time-varying exposures and confounding to fall within a causal framework. As new ADFs come to market, patients may be switched between ADFs or comparators, creating methodological complications. We are evaluating the relative importance of applying advanced epidemiologic methods for assessing time-varying causation in claims-based studies similar to those found in post-marketing requirements. To accomplish this, we will first use claims-linked-mortality-data to ascertain whether ADF patients have different risk profiles than non-ADF opioid patients. Then, we will apply statistical methods to account for time-varying confounding.

A critical conceptual component of this Task is that a patient’s risk profile will change over time, as new behaviors emerge and are documented in the medical record. To account for these time-varying issues we will adjust for observable time-fixed and time- varying confounding by utilizing two approaches: (1) g- estimation of structural nested cumulative failure time models (SNCFTM); and (2) inverse probability weighted marginal structural models (MSM). Both of these methods utilize the counterfactual or potential outcomes framework. G-estimation of SNCFTM allows comparison of counterfactual outcomes within the categories of observed baseline and time-varying covariates (structural nested models), thereby allowing a direct contrast of counterfactual outcomes for the observed time-varying exposure categories. These models can be fit for both categorical and continuous time- varying exposures. On the other hand, inverse probability weighted MSM account for time-varying confounding by comparing counterfactual outcomes for the entire observed population (i.e., potential outcomes if the entire population had a certain defined level of exposure compared to if the same entire population had another defined level of exposure). Both methods can be readily implemented in standard statistical software packages.
Controlling for time-varying confounding using traditional regression methods is inappropriate because such methods block causal effects of the exposure on the outcome and introduce collider-stratification (selection) bias. This fact has been overlooked in much of the opioids research published to date.There are paradoxical pitfalls in ADF evaluation using longitudinal data which pose a threat to validity of PMR studies. For example, it may appear intuitive to adjust for pain severity to control for differences between ADF recipients and comparators. Collider stratification bias arises because of time-varyingness: exposure X1 (at time 1) affects outcome Y1 (at time 1), and both X and Y are affected by the confounder pain symptoms (C1). All three of these have an effect on the exposure at time 2 (X2), and the pain symptoms at time 2 (C2). All of which also affect the outcome Y2. If we were to adjust on pain scores (C1 and C2), we actually open up many of the collider pathways between C1, X1, and Y1, thereby causing selection bias. 

## How to use the results
We will explore the importance of using methods that account for time- varying confounding and create tutorials and computer code that can been applied by other researchers to account for these limitations. Our use of SNCFTM and MSM will account for time-varying exposure and confounding to provide the total effect of all ADF usage over time. We will produce publicly available explanatory tutorials and computer code to explain the problem and provide an applied solution for a wider audience.

## Who is conducting and supporting  the study
Shabbar Ranapurwala, Becky Naumann, Steve Marshall, Daniela Moga, Svetla Slavova, Maryalice Nocera and Nabarun Dasgupta will be supported by database programmers. The study is being conducted jointly by the University of North Carolina and University of Kentucky.  Funding for data analysis is from the United States Food and Drug Administration. All studies at the Opioid Data Lab are conducted by independent researchers at the University of Kentucky and the University of North Carolina at Chapel Hill, and do not necessarily represent the views of funders or partners. We are grateful to generations of taxpayers in Kentucky and North Carolina for supporting public universities. We are also grateful to US taxpayers for safeguarding public health by supporting FDA and this research project. This study has been registered with the University of North Carolina Institutional Review Board.


 
